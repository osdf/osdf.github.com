<!DOCTYPE html>
<html lang="en">
    <head>
        <title>osdf's log : Training not on MNIST</title>
        <meta charset="utf-8" />
        <link rel="stylesheet" href="http://osdf.github.com/blog/theme/css/main.css" type="text/css" />

        <!--[if IE]>
                <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

        <!--[if lte IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="http://osdf.github.com/blog/css/ie.css"/>
                <script src="http://osdf.github.com/blog/js/IE8.js" type="text/javascript"></script><![endif]-->

        <!--[if lt IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="http://osdf.github.com/blog/css/ie6.css"/><![endif]-->
        <script type="text/javascript">

          var _gaq = _gaq || [];
          _gaq.push(['_setAccount', 'UA-35562047-1']);
          _gaq.push(['_trackPageview']);

          (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
          })();

        </script>
</head>

<body>
        
<header>
    <h1>Training not on MNIST</h1>
<time datetime="2012-11-29T00:00:00">Thu 29 November 2012</time></header>
<article>
    <p><a href="http://yaroslavvb.blogspot.de/">Yaroslav Bulatov</a> made a dataset of glyphs available that he called <a href="http://yaroslavvb.blogspot.de/2011/09/notmnist-dataset.html">notMNIST</a>.
I briefly ran my implementation for deep networks on the dataset and got about
3.8% error rate on the test set. For the <em>inverted</em> data (binary pixel inversion)
I get a similar error, about 4% (inverted <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> is more difficult than the original version,
at least when one does not take precaution, e.g. see a nice recent <a href="http://www.cs.toronto.edu/~tang/papers/RbmZM.pdf">report</a>
that talks about MNIST and binary-binary RBMs.)</p>
<p>The network is a 784-1024-1024-1024-1024-10 neural network. All layers except
the last one are pretrained as a binary-binary RBM: 50 epochs of minibatch gradient
descent with a learning rate of 1e-3. Gradient computation for the RBMs is done via CD-1.
Afterwards, the deep network is finetuned for 500 epochs with a learning rate of 2.5e-3,
again with minibatch gradient descent. The input is not preprocessed.</p>
<p>Important note: I didn't use any validation set to tune the learning rate(s) and the
other hyperparameters: I chose the learning rate for the fine tuning part such 
that a smooth progress on the training set is achieved. Stoping after 500 epochs
is completely arbitrary &#8212; actually, I didn't want to wait longer.
If time permits I will do a more thorough set of experiments using a validation set 
to determine hyperparameters.</p>
</article>

        <footer>
        <p>
        <em><a href="http://osdf.github.com">osdf</a> is on <a href="http://github.com/osdf">github</a>, <a href="https://twitter.com/plastiq">twitter</a> and <a href="http://de.linkedin.com/pub/christian-osendorfer/22/915/a36">linkedin</a>. This page is part of his <a href="http://osdf.github.com/blog/index.html">blog</a>.<em>
        </footer>

</body>
</html>